{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:17<00:00,  2.17s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 23.69 GiB total capacity; 16.87 GiB already allocated; 66.94 MiB free; 16.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/root/sunyd/codes/LLMS/ChatGLM-6Béƒ¨ç½²ä¸å¾®è°ƒæ•™ç¨‹/1 ChatGLM æ¨ç†æµ‹è¯•.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.198/root/sunyd/codes/LLMS/ChatGLM-6B%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BE%AE%E8%B0%83%E6%95%99%E7%A8%8B/1%20ChatGLM%20%E6%8E%A8%E7%90%86%E6%B5%8B%E8%AF%95.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/root/sunyd/model_hub/chatglm-6b/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.198/root/sunyd/codes/LLMS/ChatGLM-6B%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BE%AE%E8%B0%83%E6%95%99%E7%A8%8B/1%20ChatGLM%20%E6%8E%A8%E7%90%86%E6%B5%8B%E8%AF%95.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.198/root/sunyd/codes/LLMS/ChatGLM-6B%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BE%AE%E8%B0%83%E6%95%99%E7%A8%8B/1%20ChatGLM%20%E6%8E%A8%E7%90%86%E6%B5%8B%E8%AF%95.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_path, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mhalf()\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.198/root/sunyd/codes/LLMS/ChatGLM-6B%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BE%AE%E8%B0%83%E6%95%99%E7%A8%8B/1%20ChatGLM%20%E6%8E%A8%E7%90%86%E6%B5%8B%E8%AF%95.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/sunyd/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/miniconda3/envs/sunyd/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sunyd/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sunyd/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sunyd/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/sunyd/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 23.69 GiB total capacity; 16.87 GiB already allocated; 66.94 MiB free; 16.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "model_path = \"/root/sunyd/model_hub/chatglm-6b/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n",
      "[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚')]\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n",
    "print(response)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¦‚æœæ™šä¸Šç¡ä¸ç€ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š\n",
      "\n",
      "1. ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´ï¼šå°½é‡åœ¨åŒä¸€æ—¶é—´ä¸ŠåºŠç¡è§‰ï¼Œå¹¶ç¡®ä¿æ¯å¤©ä¿æŒç›¸åŒçš„èµ·åºŠæ—¶é—´ã€‚\n",
      "\n",
      "2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒï¼šä¿æŒæˆ¿é—´å®‰é™ã€é»‘æš—ã€å‡‰çˆ½ï¼Œå¹¶ç¡®ä¿åºŠå’ŒåºŠå«èˆ’é€‚ã€‚\n",
      "\n",
      "3. æ”¾æ¾èº«ä½“å’Œå¤´è„‘ï¼šåœ¨ç¡å‰è¿›è¡Œä¸€äº›æ”¾æ¾æ´»åŠ¨ï¼Œå¦‚å†¥æƒ³ã€æ·±å‘¼å¸ã€æ¸©æ°´æ³¡è„šæˆ–å¬æŸ”å’Œçš„éŸ³ä¹ã€‚\n",
      "\n",
      "4. é¿å…åœ¨ç¡å‰åƒè¿‡å¤šçš„é£Ÿç‰©æˆ–é¥®æ–™ï¼šé¿å…åœ¨ç¡å‰åƒè¿‡å¤šçš„é£Ÿç‰©æˆ–é¥®æ–™ï¼Œç‰¹åˆ«æ˜¯å’–å•¡å› ã€é…’ç²¾å’Œå«ç³–é¥®æ–™ã€‚\n",
      "\n",
      "5. é¿å…åœ¨åºŠä¸Šè¿›è¡Œåˆºæ¿€æ€§æ´»åŠ¨ï¼šé¿å…åœ¨åºŠä¸Šè¿›è¡Œåˆºæ¿€æ€§æ´»åŠ¨ï¼Œå¦‚çœ‹ç”µè§†ã€ä½¿ç”¨ç”µè„‘æˆ–æ‰‹æœºã€‚\n",
      "\n",
      "6. å°è¯•è¿›è¡Œä¸€äº›ä¼¸å±•è¿åŠ¨ï¼šåœ¨ç¡å‰è¿›è¡Œä¸€äº›ä¼¸å±•è¿åŠ¨ï¼Œå¯ä»¥ç¼“è§£èº«ä½“ç–²åŠ³ï¼Œå¸®åŠ©å…¥ç¡ã€‚\n",
      "\n",
      "å¦‚æœè¿™äº›æ–¹æ³•ä¸èƒ½å¸®åŠ©å…¥ç¡ï¼Œå»ºè®®å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶ï¼Œè·å–æ›´ä¸“ä¸šçš„å»ºè®®ã€‚\n",
      "[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'), ('æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ', 'å¦‚æœæ™šä¸Šç¡ä¸ç€ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š\\n\\n1. ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´ï¼šå°½é‡åœ¨åŒä¸€æ—¶é—´ä¸ŠåºŠç¡è§‰ï¼Œå¹¶ç¡®ä¿æ¯å¤©ä¿æŒç›¸åŒçš„èµ·åºŠæ—¶é—´ã€‚\\n\\n2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒï¼šä¿æŒæˆ¿é—´å®‰é™ã€é»‘æš—ã€å‡‰çˆ½ï¼Œå¹¶ç¡®ä¿åºŠå’ŒåºŠå«èˆ’é€‚ã€‚\\n\\n3. æ”¾æ¾èº«ä½“å’Œå¤´è„‘ï¼šåœ¨ç¡å‰è¿›è¡Œä¸€äº›æ”¾æ¾æ´»åŠ¨ï¼Œå¦‚å†¥æƒ³ã€æ·±å‘¼å¸ã€æ¸©æ°´æ³¡è„šæˆ–å¬æŸ”å’Œçš„éŸ³ä¹ã€‚\\n\\n4. é¿å…åœ¨ç¡å‰åƒè¿‡å¤šçš„é£Ÿç‰©æˆ–é¥®æ–™ï¼šé¿å…åœ¨ç¡å‰åƒè¿‡å¤šçš„é£Ÿç‰©æˆ–é¥®æ–™ï¼Œç‰¹åˆ«æ˜¯å’–å•¡å› ã€é…’ç²¾å’Œå«ç³–é¥®æ–™ã€‚\\n\\n5. é¿å…åœ¨åºŠä¸Šè¿›è¡Œåˆºæ¿€æ€§æ´»åŠ¨ï¼šé¿å…åœ¨åºŠä¸Šè¿›è¡Œåˆºæ¿€æ€§æ´»åŠ¨ï¼Œå¦‚çœ‹ç”µè§†ã€ä½¿ç”¨ç”µè„‘æˆ–æ‰‹æœºã€‚\\n\\n6. å°è¯•è¿›è¡Œä¸€äº›ä¼¸å±•è¿åŠ¨ï¼šåœ¨ç¡å‰è¿›è¡Œä¸€äº›ä¼¸å±•è¿åŠ¨ï¼Œå¯ä»¥ç¼“è§£èº«ä½“ç–²åŠ³ï¼Œå¸®åŠ©å…¥ç¡ã€‚\\n\\nå¦‚æœè¿™äº›æ–¹æ³•ä¸èƒ½å¸®åŠ©å…¥ç¡ï¼Œå»ºè®®å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶ï¼Œè·å–æ›´ä¸“ä¸šçš„å»ºè®®ã€‚')]\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n",
    "print(response)\n",
    "print(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sunyd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
